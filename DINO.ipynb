{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA1qrkLYZgWP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!pip install vit_pytorch\n",
        "from vit_pytorch import ViT, Dino\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToTensor\n",
        "from skimage import io\n",
        "from torchvision import datasets, transforms, models\n",
        "import tarfile\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import random_split, TensorDataset, Dataset\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
        "test_img = torch.randn(1, 3, 224, 224)\n",
        "dinov2_vits14(test_img).shape"
      ],
      "metadata": {
        "id": "njgub1AJ_IAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for name, layer in dinov2_vits14.named_modules():\n",
        "  #print(layer)\n",
        "\n",
        "learnable_modules = ['blocks.8',\n",
        "                    'blocks.9',\n",
        "                    'blocks.10',\n",
        "                    'blocks.11']\n",
        "dinov2_vits14.patch_embed.proj = nn.Conv2d(4, 384, kernel_size=(14, 14), stride=(14, 14))\n",
        "dinov2_vits14.requires_grad_(False)\n",
        "modules = dict(dinov2_vits14.named_modules())\n",
        "for name in learnable_modules:\n",
        "    modules[name].requires_grad_(True)"
      ],
      "metadata": {
        "id": "3cpMml44KyVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class L2Norm(nn.Module):\n",
        "    def forward(self, x, eps = 1e-6):\n",
        "        norm = x.norm(dim = 1, keepdim = True).clamp(min = eps)\n",
        "        return x / norm\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, dim, num_classes):\n",
        "    super().__init__()\n",
        "    self.layer = nn.Linear(dim, num_classes)\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)"
      ],
      "metadata": {
        "id": "W1kTEQIKBB_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/oxford-102-flowers.tgz\"\n",
        "download_url(dataset_url, '.')\n",
        "\n",
        "with tarfile.open('./oxford-102-flowers.tgz', 'r:gz') as tar:\n",
        "    tar.extractall(path='./data')\n",
        "\n",
        "img_path='./data/oxford-102-flowers/jpg/image_00001.jpg'\n",
        "\n",
        "class flowersmodel(Dataset):\n",
        "  def __init__(self,excel_file,root_dir,transform=None):\n",
        "    self.annotations=pd. read_csv(excel_file,delimiter=' ')\n",
        "    self.root_dir=root_dir\n",
        "    self.transform=transform\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "  def __getitem__(self,index):\n",
        "    img_path=os.path.join(self.root_dir,self.annotations.iloc[index,0])\n",
        "    image=io.imread(img_path)\n",
        "    y_label=torch.tensor(self.annotations.iloc[index,1])\n",
        "    image=Image.open(img_path).resize((300,300),resample=0)\n",
        "    if self.transform:\n",
        "      image=self.transform(image)\n",
        "    return (image,y_label)\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomRotation(75),\n",
        "        transforms.Resize(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'testing': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "\n",
        "train_dataset=flowersmodel('./data/oxford-102-flowers/train.txt',root_dir='./data/oxford-102-flowers',transform=data_transforms['train'])\n",
        "test_dataset=flowersmodel('./data/oxford-102-flowers/test.txt',root_dir='./data/oxford-102-flowers',transform=data_transforms['testing'])\n",
        "val_dataset=flowersmodel('./data/oxford-102-flowers/valid.txt',root_dir='./data/oxford-102-flowers',transform=data_transforms['validation'])\n",
        "\n"
      ],
      "metadata": {
        "id": "fxQEYE0XlZER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {\n",
        "    'training' : torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True),\n",
        "    'testing' : torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False),\n",
        "    'validation' : torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "}"
      ],
      "metadata": {
        "id": "27xiZmwmlyUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display(inp):\n",
        "  B, C, H, W = inp.shape\n",
        "  if (C != 3):\n",
        "    inp = inp.permute(0, 3, 2, 1)\n",
        "  plt.imshow(inp)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "xFUj1Cd8ZqSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT_Dino(nn.Module):\n",
        "  def __init__(self, num_classes, in_feats=384):\n",
        "    super().__init__()\n",
        "    self.encoder = dinov2_vits14\n",
        "    self.classifier = MLP(in_feats, num_classes)\n",
        "  def forward(self, x):\n",
        "    #x = L2Norm(x)\n",
        "    x = self.encoder(x)\n",
        "    return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "J7vtkfDSaGjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import os\n",
        "device = \"cuda:0\"\n",
        "def pretrain(model, learner, train_loader, optimizer, epochs, PATH=None):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  for epoch in tqdm(range(1, epochs + 1)):\n",
        "    check = 0\n",
        "    for images, labels in tqdm(train_loader, leave=False):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      loss = learner(images)\n",
        "      avg_loss += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      learner.update_moving_average()\n",
        "      if check % 100 == 0:\n",
        "        print(loss.detach())\n",
        "    check += 1\n",
        "  if PATH:\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "\n",
        "def compute_accuracy(model, loader):\n",
        "    total_correct = 0\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    for inputs, labels in tqdm(loader, leave=False):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        output = model(inputs)\n",
        "        _, pred = torch.max(output, 1)\n",
        "        for d in zip(pred, labels):\n",
        "          if d[0].item() == d[1].item():\n",
        "            total_correct += 1\n",
        "    return total_correct / len(loader.dataset)\n",
        "\n",
        "def finetune(model, train_loader, val_loader, num_epochs, criterion, optimizer, path=None, scheduler=None, pretrained=False, pretrained_path=None):\n",
        "    print('beginning to train model')\n",
        "    if path and not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "    if pretrained and (pretrained_path is not None):\n",
        "      model.load_state_dict(torch.load(pretrained_path))\n",
        "    model.to(device)\n",
        "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
        "        total_loss = 0\n",
        "        start_time = time.perf_counter()\n",
        "        model.train()\n",
        "        for inputs, labels in tqdm(train_loader, leave=False):\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = model(inputs)\n",
        "          loss = criterion(output, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss\n",
        "        if path:\n",
        "          torch.save(model.state_dict(), f'{path}/model_ep_{epoch:02d}.pth')\n",
        "        end_time = time.perf_counter()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        train_acc = compute_accuracy(model, val_loader)\n",
        "\n",
        "\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        if scheduler and current_lr > 5e-5:\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f'epoch {epoch:2}',\n",
        "              f'loss: {total_loss:.3f}',\n",
        "              f'time: {duration:.3f}',\n",
        "              f'val acc: {train_acc:.4f}',\n",
        "              sep='\\t')"
      ],
      "metadata": {
        "id": "hJFDhRBSa78P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category = {}\n",
        "cat_to_name = {\"21\": \"fire lily\", \"3\": \"canterbury bells\", \"45\": \"bolero deep blue\", \"1\": \"pink primrose\", \"34\": \"mexican aster\", \"27\": \"prince of wales feathers\", \"7\": \"moon orchid\", \"16\": \"globe-flower\", \"25\": \"grape hyacinth\", \"26\": \"corn poppy\", \"79\": \"toad lily\", \"39\": \"siam tulip\", \"24\": \"red ginger\", \"67\": \"spring crocus\", \"35\": \"alpine sea holly\", \"32\": \"garden phlox\", \"10\": \"globe thistle\", \"6\": \"tiger lily\", \"93\": \"ball moss\", \"33\": \"love in the mist\", \"9\": \"monkshood\", \"102\": \"blackberry lily\", \"14\": \"spear thistle\", \"19\": \"balloon flower\", \"100\": \"blanket flower\", \"13\": \"king protea\", \"49\": \"oxeye daisy\", \"15\": \"yellow iris\", \"61\": \"cautleya spicata\", \"31\": \"carnation\", \"64\": \"silverbush\", \"68\": \"bearded iris\", \"63\": \"black-eyed susan\", \"69\": \"windflower\", \"62\": \"japanese anemone\", \"20\": \"giant white arum lily\", \"38\": \"great masterwort\", \"4\": \"sweet pea\", \"86\": \"tree mallow\", \"101\": \"trumpet creeper\", \"42\": \"daffodil\", \"22\": \"pincushion flower\", \"2\": \"hard-leaved pocket orchid\", \"54\": \"sunflower\", \"66\": \"osteospermum\", \"70\": \"tree poppy\", \"85\": \"desert-rose\", \"99\": \"bromelia\", \"87\": \"magnolia\", \"5\": \"english marigold\", \"92\": \"bee balm\", \"28\": \"stemless gentian\", \"97\": \"mallow\", \"57\": \"gaura\", \"40\": \"lenten rose\", \"47\": \"marigold\", \"59\": \"orange dahlia\", \"48\": \"buttercup\", \"55\": \"pelargonium\", \"36\": \"ruby-lipped cattleya\", \"91\": \"hippeastrum\", \"29\": \"artichoke\", \"71\": \"gazania\", \"90\": \"canna lily\", \"18\": \"peruvian lily\", \"98\": \"mexican petunia\", \"8\": \"bird of paradise\", \"30\": \"sweet william\", \"17\": \"purple coneflower\", \"52\": \"wild pansy\", \"84\": \"columbine\", \"12\": \"colt's foot\", \"11\": \"snapdragon\", \"96\": \"camellia\", \"23\": \"fritillary\", \"50\": \"common dandelion\", \"44\": \"poinsettia\", \"53\": \"primula\", \"72\": \"azalea\", \"65\": \"californian poppy\", \"80\": \"anthurium\", \"76\": \"morning glory\", \"37\": \"cape flower\", \"56\": \"bishop of llandaff\", \"60\": \"pink-yellow dahlia\", \"82\": \"clematis\", \"58\": \"geranium\", \"75\": \"thorn apple\", \"41\": \"barbeton daisy\", \"95\": \"bougainvillea\", \"43\": \"sword lily\", \"83\": \"hibiscus\", \"78\": \"lotus lotus\", \"88\": \"cyclamen\", \"94\": \"foxglove\", \"81\": \"frangipani\", \"74\": \"rose\", \"89\": \"watercress\", \"73\": \"water lily\", \"46\": \"wallflower\", \"77\": \"passion flower\", \"51\": \"petunia\"}\n",
        "for key in cat_to_name:\n",
        "  k = int(key) - 1\n",
        "  category[f\"{k}\"] = cat_to_name[f'{key}']"
      ],
      "metadata": {
        "id": "1rzFAsjMl-3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "PATH = 'DINO_ViT_test.pth'\n",
        "MODEL_PATH = \"Finetuned_DINO_Vit.pth\"\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "vit = ViT_Dino(102)\n",
        "optimizer = torch.optim.Adam(vit.parameters(), lr=1e-4)\n",
        "\n",
        "finetune(vit, dataloaders['training'], dataloaders['validation'], 25, criterion, optimizer, MODEL_PATH, False)"
      ],
      "metadata": {
        "id": "ZBz2Ec3fmVrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "count = 0\n",
        "for inputs, labels in dataloaders['testing']:\n",
        "  vit.cuda()\n",
        "  inputs, labels = inputs.cuda(), labels.cuda()\n",
        "  output = vit(inputs)\n",
        "  _, pred = torch.max(output, 1)\n",
        "  predicted = pred.item()\n",
        "  ground_truth = labels.item()\n",
        "  print(ground_truth, predicted)"
      ],
      "metadata": {
        "id": "3TRG_fuwPEko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"\" #path to a dowloaded .jpg test image\n",
        "image = Image.open('/content/drive/MyDrive/COSMOSBIPINNATUSSensationCosmo_1.jpg').resize((300,300),resample=0)\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "img_trans = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "image = img_trans(image)\n",
        "image = torch.reshape(image, (1, 3, 224, 224))\n",
        "plt.imshow(torch.reshape(image, (224, 224, 3)))\n",
        "plt.show()\n",
        "vit.eval()\n",
        "vit.cuda()\n",
        "image = image.cuda()\n",
        "output = vit(image)\n",
        "_, pred = torch.max(output, 1)\n",
        "print(pred)\n",
        "print(\"MODEL PREDICTION: \", category[f\"{pred.item()}\"])"
      ],
      "metadata": {
        "id": "_VhMg5LMJZz_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}